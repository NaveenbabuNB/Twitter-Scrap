{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "079fe0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 10:33:52.433 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/apple/opt/anaconda3/lib/python3.9/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from PIL import Image\n",
    "from datetime import date\n",
    "import json\n",
    "\n",
    "# *\"Connection String will be done only if we connect database\"*\n",
    "client = pymongo.MongoClient(\"mongodb+srv://NaveenNB10:<Naveen@1009>@naveenbabu.1pfbszp.mongodb.net/test/?retryWrites=true&w=majority\")\n",
    "tweetdb = client.naveen\n",
    "tweetdb_main = tweetdb.twitterscr.py\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "  tweets = 20   \n",
    "  st.markdown(\"\"\"\n",
    "  <style>\n",
    "   body {   \n",
    "   background-color: #00f900 !important;\n",
    "   }\n",
    "   </style>\n",
    "    \"\"\", \n",
    "   unsafe_allow_html=True) \n",
    "    \n",
    "  st.markdown(\n",
    "  f\"\"\"\n",
    "  <h1 style='color: #F63366; font-size: 48px;'>Twitter Scraping</h1>\n",
    "  \"\"\",\n",
    "  unsafe_allow_html=True)\n",
    "    \n",
    "  #st.title(\"Twitter Scraping\")\n",
    "  # Menus used in Twitter Scrape web app -- 5 menus are used\n",
    "  \n",
    "  menu = [\"Home\",\"About\",\"Search\",\"Display\",\"Download\"]\n",
    "  choice = st.sidebar.selectbox(\"Menu\",menu)\n",
    "  # Menu 1 is Home page \n",
    "  if choice==\"Home\":\n",
    "  \n",
    "    st.markdown(\n",
    "    f\"\"\"\n",
    "    <h3 style='color: #F63366; font-size: 18px;'>Home</h3>\n",
    "    \"\"\",\n",
    "    unsafe_allow_html=True)\n",
    "    \n",
    "    with st.expander(\"Home\"):\n",
    "        st.write('''This app is a web-based Twitter scraping tool built with the Python library Streamlit. \n",
    "                    It allows users to search for tweets containing a specific hashtag or keyword within a given time frame. \n",
    "                    The tweets are then extracted from Twitter using the Snscrape package and saved to a MongoDB database.\n",
    "                    The app provides an easy-to-use interface that allows users to specify their search parameters and view the \n",
    "                    results in real-time. The tweets can also be downloaded in CSV or JSON format for further analysis.''')                   \n",
    "                    \n",
    "        image = Image.open(r\"/Users/apple/Downloads/Elonmusk.jpg\") \n",
    "        st.image(image, width=680, caption='Chitra-Twitter Scraping')\n",
    "        \n",
    "       \n",
    "\n",
    "  # Menu 2 is about the Twitter Scrape libraries, databases and apps\n",
    "  \n",
    "  elif choice==\"About\":\n",
    "    st.markdown(\n",
    "        f\"\"\"    \n",
    "        <h3 style='color: #F63366; font-size: 18px;'>About</h3>    \n",
    "        \"\"\",\n",
    "        unsafe_allow_html=True) \n",
    "       \n",
    "    # Info about Twitter Scrapper\n",
    "    with st.expander(\"Twitter Scrapper\"):\n",
    "      st.write('''Twitter Scrapper (or Twitter Scraper) is a type of web scraping tool used to extract data from Twitter, \n",
    "                    a popular social media platform. A Twitter scraper uses automated bots to collect large amounts of\n",
    "                    data from Twitter, such as tweets, user profiles, hashtags, and trends.\n",
    "                    Twitter scrapers can be used for a wide range of applications, such as sentiment analysis, \n",
    "                    brand monitoring, market research, and social media analytics. By extracting data from Twitter, \n",
    "                    businesses and organizations can gain insights into consumer behavior, market trends, and public opinion.''')\n",
    "      image = Image.open(r\"/Users/apple/Downloads/Twitter Scrapping.png\") \n",
    "      st.image(image, caption='')\n",
    "\n",
    "    # Info about Snscraper\n",
    "    with st.expander(\"Snscraper\"):\n",
    "      st.write('''Both Snscrape and Snscraper are valid Python packages used for web scraping social media platforms, including Twitter, \n",
    "                    Instagram, and Reddit. However, the correct name of the package is Snscrape (without the extra \"r\").\n",
    "                    In some cases, people may use the incorrect name Snscraper by mistake, or they may be referring to a \n",
    "                    different package with a similar name. However, if you want to use the package, you should install \n",
    "                    and import it using the correct name Snscrape..''')\n",
    "      image = Image.open(r\"/Users/apple/Downloads/Snscrapper.png\") \n",
    "      st.image(image, caption='')\n",
    "\n",
    "\n",
    "    # Info about MongoDB database\n",
    "    with st.expander(\"MongoDB\"):\n",
    "      st.write('''MongoDB is a popular open-source, document-oriented NoSQL database that is used to store and manage \n",
    "                    unstructured and semi-structured data. MongoDB uses a document data model, which means that data is stored\n",
    "                    in flexible, JSON-like documents, making it easy to handle and scale data..''')\n",
    "      image = Image.open(r\"/Users/apple/Downloads/MongoDB.png\") \n",
    "      st.image(image, caption='')\n",
    "\n",
    "    # Info about Streamlit framework\n",
    "    with st.expander(\"Streamlit\"):\n",
    "      st.write('''Streamlit is a popular open-source Python library used for building interactive web applications and data visualizations. \n",
    "                    It allows you to create custom web applications with just a few lines of Python code. Streamlit provides a simple \n",
    "                    and intuitive way to create web applications by providing ready-to-use UI components and easy-to-use APIs..''')\n",
    "      image = Image.open(r\"/Users/apple/Downloads/Streamlit.png\") \n",
    "      st.image(image, caption='')\n",
    "      \n",
    "     \n",
    "\n",
    "  # Menu 3 is a search option\n",
    "  elif choice==\"Search\":\n",
    "    st.markdown(\n",
    "        f\"\"\"    \n",
    "        <h3 style='color: #F63366; font-size: 18px;'>Search</h3>    \n",
    "        \"\"\",\n",
    "        unsafe_allow_html=True) \n",
    "    # Every time after the last tweet the database will be cleared for updating new scraping data\n",
    "    tweetdb_main.delete_many({})\n",
    "\n",
    "    # Form for collecting user input for twitter scrape\n",
    "    with st.form(key='form1'):\n",
    "      # Hashtag input\n",
    "      st.subheader(\"Tweet searching Form\")\n",
    "      st.write(\"Enter the hashtag or keyword to perform the Twitter Scraping\")\n",
    "      query = st.text_input('Hashtag or keyword')\n",
    "\n",
    "      # No of tweets for scraping\n",
    "      st.write(\"Enter the limit for the Data Scraping: Maximum limit is 1000 tweets\")\n",
    "      limit = st.number_input('Insert a number',min_value=0,max_value=1000,step=10)\n",
    "\n",
    "      # From date to end date for scraping\n",
    "      st.write(\"Enter the Starting date to scrap the tweet data\")\n",
    "      start = st.date_input('Start date')\n",
    "      end = st.date_input('End date')\n",
    "      \n",
    "      # Submit button to scrap\n",
    "      submit_button = st.form_submit_button(label=\"Tweet Scrap\")\n",
    "    \n",
    "    if submit_button:\n",
    "      st.success(f\"Tweet hashtag {query} received for scraping\".format(query))\n",
    "\n",
    "      # TwitterSearchScraper will scrape the data and insert into MongoDB database\n",
    "      for tweet in sntwitter.TwitterSearchScraper(f'from:{query} since:{start} until:{end}').get_items():\n",
    "        # To verify the limit if condition is set\n",
    "        if tweets == limit:\n",
    "          break\n",
    "        # Stores the tweet data into MongoDB until the limit  is reached\n",
    "        else:      \n",
    "          new = {\"date\":tweet.date,\"user\":tweet.user.username, \"url\":tweet.url, \"followersCount\":tweet.user.followersCount, \"friendsCount\":tweet.user.friendsCount, \"favouritesCount\":tweet.user.favouritesCount,\"mediaCount\":tweet.user.mediaCount}\n",
    "          tweetdb_main.insert_one(new)\n",
    "          tweets += 1\n",
    "      \n",
    "    # Display the total tweets scraped\n",
    "    df = pd.DataFrame(list(tweetdb_main.find()))\n",
    "    cnt = len(df)\n",
    "    st.success(f\"Total number of tweets scraped for the input query is := {cnt}\".format(cnt))\n",
    "    \n",
    "\n",
    "  # Menu 4 is for diaplying the data uploaded in MmongoDB\n",
    "  elif choice==\"Display\":\n",
    "    st.markdown(\n",
    "        f\"\"\"    \n",
    "        <h3 style='color: #F63366; font-size: 18px;'>Display</h3>    \n",
    "        \"\"\",\n",
    "        unsafe_allow_html=True) \n",
    "    # Save the documents in a dataframe\n",
    "    df = pd.DataFrame(list(tweetdb_main.find()))\n",
    "    #Dispaly the document \n",
    "    st.dataframe(df)\n",
    "       \n",
    "\n",
    "  # Menu 5 is for Downloading the scraped data as CSV or JSON\n",
    "  else:\n",
    "    col1, col2 = st.columns(2)\n",
    "\n",
    "    # Download the scraped data as CSV\n",
    "    with col1:\n",
    "      st.write(\"Download the tweet data as CSV File\")\n",
    "      # save the documents in a dataframe\n",
    "      df = pd.DataFrame(list(tweetdb_main.find()))\n",
    "      # Convert dataframe to csv\n",
    "      df.to_csv('twittercsv.csv')\n",
    "      def convert_df(data):\n",
    "        # Cache the conversion to prevent computation on every rerun\n",
    "        return df.to_csv().encode('utf-8')\n",
    "      csv = convert_df(df)\n",
    "      st.download_button(\n",
    "                        label=\"Download data as CSV\",\n",
    "                        data=csv,\n",
    "                        file_name='twitercsv.csv',\n",
    "                        mime='text/csv',\n",
    "                        )\n",
    "      st.success(\"Successfully Downloaded data as CSV\")\n",
    "\n",
    "    # Download the scraped data as JSON\n",
    "    with col2:\n",
    "      st.write(\"Download the tweet data as JSON File\")\n",
    "      # Convert dataframe to json string instead as json file \n",
    "      twtjs = df.to_json(default_handler=str).encode()\n",
    "      # Create Python object from JSON string data\n",
    "      obj = json.loads(twtjs)\n",
    "      js = json.dumps(obj, indent=4)\n",
    "      st.download_button(\n",
    "                        label=\"Download data as JSON\",\n",
    "                        data=js,\n",
    "                        file_name='twtjs.js',\n",
    "                        mime='text/js',\n",
    "                        )\n",
    "      st.success(\"Successfully Downloaded data as JSON\")\n",
    "\n",
    "# Call the main function\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf457ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
